{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Cuda 혹은 cpu를 사용하시오.\n",
    "############Write Your Code Here############\n",
    "device = 'cpu'\n",
    "############################################\n",
    "\n",
    "\n",
    "#Custom_Dataset을 정의하시오.(10점)\n",
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        #입력으로 들어온 X의 pixel값들을 0-1사이로 normalize하고 X의 shape을 (FB,C,H,W)로 변경하여 저장하여 self.X,self.y에 저장하시오.\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        ############Write Your Code Here############\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        (FB,H,W,C) = X.shape       \n",
    "        self.X = self.X/255\n",
    "        self.X = self.X.reshape(FB,C,H,W)\n",
    "        self.y = y\n",
    "        ############################################\n",
    "        \n",
    "    def __len__(self):\n",
    "        #Custom_Dataset에 저장되어있는 총 data의 개수를 result에 저장하여 반환하시오.\n",
    "        result = 0\n",
    "        ############Write Your Code Here############\n",
    "        result = self.y.shape[0]\n",
    "        ############################################\n",
    "        return result\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #self.X, self.y 에서 idx에 맞는 data를 result_X,result_y에 저장하여 반환하시오.\n",
    "        result_X,result_y = None,None\n",
    "        ############Write Your Code Here############\n",
    "        result_X = self.X[idx]\n",
    "        result_y = self.y[idx]\n",
    "        ############################################\n",
    "        return result_X,result_y\n",
    "\n",
    "    \n",
    "#torch.nn을 사용하여 아래 함수들을 작성하시오. result는 nn.Layer중 하나이고 result를 반환함.(20점)\n",
    "def batch_norm(dim,for_MLP=True):\n",
    "    #for_MLP가 True일 시 MLP를 위한 BN Layer를 반환하고 False일 시 CNN을 위한 BN Layer를 반환함.\n",
    "    ############Write Your Code Here############\n",
    "    if for_MLP==True:\n",
    "        result=nn.BatchNorm1d(dim)\n",
    "    else:\n",
    "        result=nn.BatchNorm2d(dim)\n",
    "    # conv는 4차원(a, b, c, d)\n",
    "    # mlp는 3차원(a, b, c) \n",
    "    # 이기에 다른 놈을 써야한다.\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "def fc_layer(in_dim,out_dim):\n",
    "    #Fully Connected Layer(Dense Layer)\n",
    "    ############Write Your Code Here############\n",
    "    result=nn.Linear(in_dim, out_dim)\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "def conv_layer(in_ch,out_ch,kernel_size, stride=1, padding=0):\n",
    "    #Convolutional Layer for image\n",
    "    ############Write Your Code Here############\n",
    "    result = nn.Conv2d(in_channels=in_ch, \n",
    "              out_channels=out_ch, \n",
    "              kernel_size=kernel_size,\n",
    "              stride=stride, \n",
    "              padding=padding)\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "def relu():\n",
    "    #ReLU function\n",
    "    ############Write Your Code Here############\n",
    "    result = nn.ReLU()\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "def flatten():\n",
    "    #Flatten the data\n",
    "    ############Write Your Code Here############\n",
    "    result = nn.Flatten()\n",
    "    ############################################\n",
    "    return result\n",
    "\n",
    "\n",
    "#skip_connection(bn -> relu -> conv -> bn -> relu -> conv)를 따르는 Res_block을 만드시오.\n",
    "#change_res가 True인 res_block을 통과한 feature map은 resolution이 2배 작아지고 channel의 깊이는 2배로 증가함. ex) 32*8*8 -> 64*4*4\n",
    "#위의 경우에는 skip_connection의 dimension은 1*1 conv로 맞춰줌.\n",
    "#change_res가 False인 Res_block을 통과한 feature map은 resolution과 channel의 깊이는 그대로 유지됨. ex) 32*4*4 -> 32*4*4(20점)\n",
    "class Res_block(nn.Module):\n",
    "    def __init__(self, input_channel, change_res):\n",
    "        super(Res_block,self).__init__()\n",
    "        self.change_res = change_res\n",
    "#         if change_res:\n",
    "            ############Write Your Code Here############\n",
    "            \n",
    "            ############################################\n",
    "#         else:\n",
    "            ############Write Your Code Here############\n",
    "            \n",
    "            ############################################\n",
    "        ############Write Your Code Here############\n",
    "            \n",
    "        ############################################\n",
    "    def forward(self,X):\n",
    "        ############Write Your Code Here############\n",
    "        \n",
    "        ############################################\n",
    "        return X\n",
    "\n",
    "    \n",
    "#Skip Connection을 이용하여 20개 이상의 layer를 가지고 테스트 셋에대하여 50% 이상의 성능을 주는 MLP를 만드시오.\n",
    "#nn.ModuleList를 사용하면 많을 층의 layer를 쌓는데 용이함.(20점)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP,self).__init__()\n",
    "        ############Write Your Code Here############\n",
    "        self.btl = batch_norm(input_dim, for_MLP=False)\n",
    "        self.hd1 = fc_layer(32, 32)\n",
    "        self.hd2 = fc_layer(32, 64)\n",
    "        self.hd3 = fc_layer(64, 64)\n",
    "        self.hd4 = fc_layer(64, 64)\n",
    "        self.hd5 = fc_layer(64, 64)\n",
    "        self.hd6 = fc_layer(64, 128)\n",
    "        self.hd7 = fc_layer(128, 256)\n",
    "        self.hd8 = fc_layer(256, 256)\n",
    "        self.hd9 = fc_layer(256, 512)\n",
    "        self.hd10 = fc_layer(512, 256)\n",
    "        self.hd11 = fc_layer(256, 256)\n",
    "        self.hd12 = fc_layer(256, 256)\n",
    "        self.hd13 = fc_layer(256, 256)\n",
    "        self.hd14 = fc_layer(256, 256)\n",
    "        self.hd15 = fc_layer(256, 256)\n",
    "        self.hd16 = fc_layer(256, 256)\n",
    "        self.hd17 = fc_layer(256, 64)\n",
    "        self.hd18 = fc_layer(64, 32)\n",
    "        self.layer = fc_layer(32, output_dim)\n",
    "        ############################################\n",
    "    def forward(self,X):\n",
    "        ############Write Your Code Here############\n",
    "        X = nn.functional.relu(self.btl(X))\n",
    "#         X = nn.ReLU(X)\n",
    "        X = nn.functional.relu(self.hd1(X))\n",
    "        X = nn.functional.relu(self.hd2(X))\n",
    "        X = nn.functional.relu(self.hd3(X))\n",
    "        X = nn.functional.relu(self.hd4(X))\n",
    "        X = nn.functional.relu(self.hd5(X))\n",
    "        X = nn.functional.relu(self.hd6(X))\n",
    "        X = nn.functional.relu(self.hd7(X))\n",
    "        X = nn.functional.relu(self.hd8(X))\n",
    "        X = nn.functional.relu(self.hd9(X))\n",
    "        X = nn.functional.relu(self.hd10(X))\n",
    "        X = nn.functional.relu(self.hd11(X))\n",
    "        X = nn.functional.relu(self.hd12(X))\n",
    "        X = nn.functional.relu(self.hd13(X))\n",
    "        X = nn.functional.relu(self.hd14(X))\n",
    "        X = nn.functional.relu(self.hd15(X))\n",
    "        X = nn.functional.relu(self.hd16(X))\n",
    "        X = nn.functional.relu(self.hd17(X))\n",
    "        X = nn.functional.relu(self.hd18(X))\n",
    "        \n",
    "        X =torch.nn.functional.log_softmax(self.X)\n",
    "        ############################################\n",
    "        return X\n",
    "        \n",
    "#Res_Block을 사용하여 테스트 셋에대한 70% 이상의 성능을 주는 CNN 모델을 만드시오.\n",
    "#flatten전에 nn.AdaptiveAvgPool2d를 사용하면 dimension맞추기가 쉬움.(20점)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_channel, class_number, block_number):\n",
    "        super(CNN,self).__init__()\n",
    "        ############Write Your Code Here############\n",
    "        \n",
    "        ############################################\n",
    "    def forward(self,X):\n",
    "        ############Write Your Code Here############\n",
    "        \n",
    "        ############################################\n",
    "        return X\n",
    "\n",
    "#loader에 있는 모든 data들에 대한 정확도를 구하여 accuracy에 저장하여 accuracy를 return하는 함수를 구현하시오.(10점)\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    accuracy = 0\n",
    "    total_example = 0\n",
    "    correct_example = 0\n",
    "    for data in loader:\n",
    "        x,y = data\n",
    "        x = torch.tensor(x, device = device)\n",
    "        y = torch.tensor(y, device = device)\n",
    "        ############Write Your Code Here############\n",
    "        output = model(x)\n",
    "        test_loss += nn.functional.nll_loss(output, y, size_average=False).data\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(x.data.view_as(pred)).cpu().sum\n",
    "        ############################################\n",
    "    ############Write Your Code Here############\n",
    "    accuracy /= len(loader.dataset)\n",
    "    ############################################\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "#epoch마다 train_loader에 있는 batch들을 사용하여 모델을 학습하고\n",
    "#epoch의 마지막 iteration에서는 모델의 validation accuracy를 확인하여 제일 높은 val. acc.를 가진 model을 best_model에 저장하고\n",
    "#val_acc에는 매 epoch마다 구해진 validation accuracy를 저장하시오.\n",
    "#running_loss에는 각각의 epoch에서 모든 batch의 loss를 다 더하여 저장하시오.\n",
    "#모든 epoch의 validation accuracy를 val_acc에 저장하여 best_model과 val_acc를 return하는 함수를 구현하시오.(10점)\n",
    "def train(model, epoches, train_loader, val_loader, optimizer, criteria):\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    batch_len = len(train_loader)\n",
    "    val_acc = []\n",
    "    for epoch in range(epoches):\n",
    "        running_loss = 0\n",
    "        for i,data in enumerate(train_loader):\n",
    "            x,y = data\n",
    "            x = torch.tensor(x, device = device)\n",
    "            y = torch.tensor(y, device = device)\n",
    "            ############Write Your Code Here############\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            best_model = output.data.max(1)[1]\n",
    "            ############################################\n",
    "            \n",
    "            #epoch의 마지막 iteration\n",
    "            if i % batch_len == batch_len-1:\n",
    "                print(f'{epoch+1}th iteration loss :',running_loss/batch_len)\n",
    "                running_loss = 0\n",
    "                ############Write Your Code Here############\n",
    "                \n",
    "                ############################################\n",
    "    return best_model, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:225: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLP' object has no attribute 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-ff897fa8ba38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-99d90cb179fb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epoches, train_loader, val_loader, optimizer, criteria)\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[1;31m############Write Your Code Here############\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;31m############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-99d90cb179fb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhd18\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;31m############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    946\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 948\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLP' object has no attribute 'X'"
     ]
    }
   ],
   "source": [
    "#(50점)\n",
    "#Read the data\n",
    "trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True)\n",
    "testset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True)\n",
    "\n",
    "X_train, Y_train = trainset.data, np.array(trainset.targets)\n",
    "X_test, Y_test = testset.data, np.array(testset.targets)\n",
    "\n",
    "\n",
    "#앞서 정의한 Custom_Dataset과 DataLoader를 사용하여 train_loader,val_loader,test_loader를 정의하시오.\n",
    "#Batch_size는 본인의 컴퓨터 사향에 맞게 변경하면 됨. Validation Set으로 Train Set의 20%를 사용함.\n",
    "#Preprocessing\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "batch_size = 1\n",
    "############Write Your Code Here############\n",
    "train_data = Custom_Dataset(X_train, Y_train)\n",
    "test_data = Custom_Dataset(X_test, X_test)\n",
    "val_size = int(len(train_data.y)/5)\n",
    "train_size = len(train_data.y) - val_size\n",
    "train_loaders, val_loaders = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
    "train_loader = DataLoader(train_loaders, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_loaders, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "############################################\n",
    "\n",
    "\n",
    "#앞서 정의한 MLP,CNN을 사용하여 mlp_model,cnn_model을 정의하시오.\n",
    "#Define the model\n",
    "mlp_model = None\n",
    "cnn_model = None\n",
    "############Write Your Code Here############\n",
    "\n",
    "mlp_model = MLP(3, 10)\n",
    "\n",
    "\n",
    "############################################\n",
    "mlp_model.to(device)\n",
    "# cnn_model.to(device)\n",
    "\n",
    "\n",
    "#앞서 정의한 train함수를 사용하여 best_mlp, mpl_val_acc, best_cnn, cnn_val_acc를 구하시오.\n",
    "#Train the model\n",
    "best_mlp = None\n",
    "mlp_val_acc = None\n",
    "best_cnn = None\n",
    "cnn_val_acc = None\n",
    "############Write Your Code Here############\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp_model.parameters(), lr=0.001, momentum=0.9)\n",
    "train(mlp_model, 1, train_loader, val_loader, optimizer, criterion)\n",
    "\n",
    "\n",
    "############################################\n",
    "\n",
    "\n",
    "#앞서 정의한 evaluate함수와 best_model들을 사용하여 mlp_acc, cnn_acc를 구하시오.\n",
    "#Test Accuracy\n",
    "mlp_acc = None  \n",
    "cnn_acc = None \n",
    "############Write Your Code Here############\n",
    "\n",
    "############################################\n",
    "print('MLP accuracy:',mlp_acc)\n",
    "print('CNN accuracy:',cnn_acc)\n",
    "\n",
    "\n",
    "#앞서 구한 val_acc들을 사용하여 이해 가능한 그래프를 그리시오.\n",
    "#Validation Accuracy Plot\n",
    "############Write Your Code Here############\n",
    "\n",
    "############################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
