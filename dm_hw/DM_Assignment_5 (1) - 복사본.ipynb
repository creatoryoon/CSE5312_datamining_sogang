{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2651,
     "status": "ok",
     "timestamp": 1622096231189,
     "user": {
      "displayName": "BokJin Chung",
      "photoUrl": "",
      "userId": "00013653044591416632"
     },
     "user_tz": -540
    },
    "id": "Hi0HXI_9M8lY",
    "outputId": "7862d8a5-07f4-4e67-a77b-4809b18144ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hc6X4fRDKP-j"
   },
   "outputs": [],
   "source": [
    "#구현하는 모델에서 쓰이는 모든 activation함수는 정의하여 드린 GELU 함수를 사용해야함.\n",
    "#MultiHeadAttention에서 Head로 나눌때, 이미지를 patch로자른후 sequence로 만들때 Rearrange함수를 사용하면 편리함.(사용하지 않으셔도 됩니다)\n",
    "#CIFAR10에 대한 test accuracy가 60프로 이상인 ViT모델을 만드시오.\n",
    "import tensorflow as tf\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "from tensorflow.keras.activations import gelu\n",
    "GELU = lambda x : gelu(x)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QrJci74oNV2m"
   },
   "outputs": [],
   "source": [
    "#논문[1]에서 설명하는 MultiHeadAttention을 만들어라.\n",
    "class MultiHeadedAttention(tf.keras.Model):\n",
    "    #dimension - 모델의 dimension(MHA를 거친 후의 dimension)\n",
    "    def __init__(self, dimension, heads=8):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        ############Write your code Here############\n",
    "        self.heads = heads\n",
    "        self.scale = dimension ** -0.5\n",
    "\n",
    "        self.mlp_in = tf.keras.layers.Dense(dimension * 3, use_bias=False)\n",
    "        self.mlp_out = tf.keras.layers.Dense(dimension)\n",
    "\n",
    "        self.rearrange_attention = Rearrange(\n",
    "            'b n (qkv h d) -> qkv b h n d', qkv=3, h=self.heads)\n",
    "        self.rearrange_output = Rearrange('b h n d -> b n (h d)')\n",
    "        \n",
    "        \n",
    "        ############################################\n",
    "    def call(self, inputs):\n",
    "        output = None\n",
    "        ############Write your code Here############\n",
    "        query_key_value = self.mlp_in(inputs)\n",
    "        query_key_value = self.rearrange_attention(query_key_value)\n",
    "\n",
    "        query = query_key_value[0]\n",
    "        key = query_key_value[1]\n",
    "        value = query_key_value[2]\n",
    "\n",
    "        dot_product = tf.einsum('bhid,bhjd->bhij', query, key) * self.scale\n",
    "        attention = tf.nn.softmax(dot_product, axis=-1)\n",
    "\n",
    "        output = tf.einsum('bhij,bhjd->bhid', attention, value)\n",
    "        output = self.rearrange_output(output)\n",
    "        output = self.mlp_out(output)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################################\n",
    "        return output\n",
    "\n",
    "#인자로 받은 residual_function을 사용하여 real_function값을 return하여주는 Class를 만들어라.(call함수 참고)\n",
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, residual_function):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        ############Write your code Here############\n",
    "        self.residual_function = residual_function\n",
    "        ############################################\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.residual_function(inputs) + inputs\n",
    "\n",
    "#인자로 받은 normfunction에 들어가기전에 LayerNormalization을 해주는 Class를 만들어라.(call함수 참고)\n",
    "class NormalizationBlock(tf.keras.Model):\n",
    "    def __init__(self, norm_function, epsilon=1e-5):\n",
    "        super(NormalizationBlock, self).__init__()\n",
    "        ############Write your code Here############\n",
    "        self.norm_function = norm_function\n",
    "        self.normalize = tf.keras.layers.LayerNormalization(epsilon=epsilon)\n",
    "        ############################################\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.norm_function(self.normalize(inputs))\n",
    "\n",
    "#논문[1]에서의 MLPBlock을 만들어라.\n",
    "class MLPBlock(tf.keras.Model):\n",
    "    #output_dimension - MLPBlock의 output dimension\n",
    "    #hidden_dimension - MLPBlock의 hidden layer dimension\n",
    "    def __init__(self, output_dimension, hidden_dimension):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        ############Write your code Here############\n",
    "        self.mlp_1 = tf.keras.layers.Dense(hidden_dimension)\n",
    "        self.mlp_2 = tf.keras.layers.Dense(output_dimension)\n",
    "\n",
    "        ############################################\n",
    "\n",
    "    def call(self, inputs):\n",
    "        output = None\n",
    "        ############Write your code Here############\n",
    "        y = self.mlp_1(inputs)\n",
    "        y = GELU(y)\n",
    "        y = self.mlp_2(y)\n",
    "        output = GELU(y)\n",
    "        ############################################\n",
    "        return output\n",
    "\n",
    "#논문[1]을 읽고 TransformerEncoder를 위에서 정의한 class들을 사용하여 만들어라.\n",
    "class TransformerEncoder(tf.keras.Model):\n",
    "    #dimension - 모델의 dimension(MHA를 거친 후의 dimension), heads - MHA에서 head의 개수\n",
    "    #depth - encoder layer의 개수, mlp_dimension - MLP block의 hidden layer의 dimension\n",
    "    def __init__(self, dimension, depth, heads, mlp_dimension): \n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        layers_ = []\n",
    "        for _ in range(depth):\n",
    "            ############Write your code Here############\n",
    "            layers_ += [\n",
    "                ResidualBlock(\n",
    "                    NormalizationBlock(\n",
    "                        dimension,\n",
    "                        MultiHeadedAttention(\n",
    "                            dimension, heads=heads\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                ResidualBlock(\n",
    "                    NormalizationBlock(\n",
    "                        dimension,\n",
    "                        MLPBlock(dimension, mlp_dimension)\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "            ############################################\n",
    "        self.layers_ = tf.keras.Sequential(layers_)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.layers_(inputs)\n",
    "\n",
    "#논문[2]를 읽고 ViT모델을 위에서 정의한 class들을 사용하여 만들어라.\n",
    "class ImageTransformer(tf.keras.Model):\n",
    "    #image_size - 이미지의 W==H의 크기(int), patch_size - 이미지를 쪼갤 patch의 크기(int)\n",
    "    #n_classes - 최종 class의 개수, batch_size - 배치사이즈\n",
    "    #dimension - 모델의 dimension(MHA를 거친 후의 dimension), depth - encoder layer의 개수\n",
    "    #heads - MHA에서 head의 개수, mlp_dimension - MLP block의 hidden layer의 dimension\n",
    "    #channel - input image에 대한 channel의 수\n",
    "    def __init__(\n",
    "            self, image_size, patch_size, n_classes, batch_size,\n",
    "            dimension, depth, heads, mlp_dimension, channels=3):\n",
    "        super(ImageTransformer, self).__init__()\n",
    "        assert image_size % patch_size == 0, 'invalid patch size for image size'\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.dimension = dimension\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            \"position_embeddings\", shape=[num_patches + 1, dimension],\n",
    "            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n",
    "        )\n",
    "        self.classification_token = self.add_weight(\n",
    "            \"classification_token\", shape=[1, 1, dimension],\n",
    "            initializer=tf.keras.initializers.RandomNormal(), dtype=tf.float32\n",
    "        )\n",
    "        ############Write your code Here############\n",
    "        self.embedding_mlp = tf.keras.layers.Dense(dimension)\n",
    "        self.rearrange = Rearrange(\n",
    "            'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "            p1=self.patch_size, p2=self.patch_size\n",
    "        )\n",
    "        self.transformer = TransformerEncoder(dimension, depth, heads, mlp_dimension)\n",
    "        self.classification_identity = tf.identity\n",
    "        self.mlp_1 = tf.keras.layers.Dense(mlp_dimension)\n",
    "        self.den = tf.keras.layers.Dense(n_classes)\n",
    "        ############################################\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        output = None\n",
    "        ############Write your code Here############\n",
    "        shapes = tf.shape(inputs)\n",
    "        y = self.rearrange(inputs)\n",
    "        y = self.embedding_mlp(y)\n",
    "        cls_tokens = tf.broadcast_to(\n",
    "            self.classification_token,\n",
    "            (shapes[0], 1, self.dimension)\n",
    "        )\n",
    "        y = tf.concat((cls_tokens, inputs), axis=1)\n",
    "        y += self.positional_embedding\n",
    "        y = self.transformer(y)\n",
    "        y = self.classification_identity(y[:, 0])\n",
    "        y = self.mlp_1(y)\n",
    "        y = GELU(y)\n",
    "        \n",
    "        output = self.den(y)\n",
    "        ############################################\n",
    "        return output\n",
    "    \n",
    "    \n",
    "#     def compile(self, loss_fn, **kwargs):\n",
    "#         super(ImageTransformer, self).compile(**kwargs)\n",
    "#         self.loss_fn = loss_fn\n",
    "#         self.train_accuracy = tf.keras.metrics.Accuracy('training_accuracy', dtype=tf.float32)\n",
    "\n",
    "    \n",
    "#     def train_step(self, data):\n",
    "#         def _step(inputs):\n",
    "#             X, Y = inputs\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 logits = self(X, training=True)\n",
    "#                 num_labels = tf.shape(logits)[-1]\n",
    "#                 label_mask = tf.math.logical_not(Y < 0)\n",
    "#                 label_mask = tf.reshape(label_mask, (-1,))\n",
    "#                 logits = tf.reshape(logits, (-1, num_labels))\n",
    "#                 logits_masked = tf.boolean_mask(logits, label_mask)\n",
    "#                 label_ids = tf.reshape(Y, (-1,))\n",
    "#                 label_ids_masked = tf.boolean_mask(label_ids, label_mask)\n",
    "#                 cross_entropy = self.loss_fn(label_ids_masked, logits_masked)\n",
    "#                 loss = tf.reduce_sum(cross_entropy) * (1.0 / self.batch_size)\n",
    "#                 y_pred = tf.argmax(tf.nn.softmax(logits, axis=-1), axis=-1)\n",
    "#                 self.train_accuracy.update_state(tf.squeeze(Y), y_pred)\n",
    "#             grads = tape.gradient(loss, self.trainable_variables)\n",
    "#             self.optimizer.apply_gradients(\n",
    "#                 list(zip(grads, self.trainable_variables)))\n",
    "#             return cross_entropy\n",
    "\n",
    "#         total_loss = self.distribute_strategy.reduce(\n",
    "#             tf.distribute.ReduceOp.SUM,\n",
    "#             self.distribute_strategy.run(_step, args=(data,)), axis=0\n",
    "#         )\n",
    "#         mean_loss = total_loss / self.batch_size\n",
    "#         return {\n",
    "#             'total_loss': total_loss,\n",
    "#             'mean_loss': mean_loss,\n",
    "#             'train_accuracy': self.train_accuracy.result()\n",
    "#         }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "cAydwOELeFba"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'master_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-55db7b5b6ad1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImageTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;31m#Initialize your model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;31m#Initialize optimizer and loss and compile it to the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-55db7b5b6ad1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mtest_pb_max_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_dataset_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mepoch_bar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaster_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepoch_bar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'master_bar' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "# Download and prepare the CIFAR10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "############Write your code Here############\n",
    "train_images = train_images / 255.\n",
    "test_images = test_images / 255.\n",
    "############################################\n",
    "# Make image shape (BS, H, W, C) to (BS, C, H, W)\n",
    "############Write your code Here############\n",
    "(height, width, channels), _ = train_images.shape[1:],train_labels.shape[1:]\n",
    "train_images = tf.cast(train_images.reshape((-1, channels, height, width)), dtype=tf.float32)\n",
    "test_images = tf.cast(test_images.reshape((-1, channels, height, width)), dtype=tf.float32)\n",
    "\n",
    "train_dataset, test_dataset = (tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(train_images),\n",
    "                                                    tf.data.Dataset.from_tensor_slices(train_labels))),\n",
    "            tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(test_images),\n",
    "                                 tf.data.Dataset.from_tensor_slices(test_labels))))\n",
    "\n",
    "############################################\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, model_config, train_dataset, train_dataset_len, test_dataset, test_dataset_len, config):\n",
    "        self.train_dataset = train_dataset.batch(config.batch_size)\n",
    "        self.train_dataset_len = train_dataset_len\n",
    "        self.test_dataset = test_dataset\n",
    "        self.test_dataset_len = None\n",
    "        self.test_dist_dataset = None\n",
    "        if self.test_dataset:\n",
    "            self.test_dataset = test_dataset.batch(config.batch_size)\n",
    "            self.test_dataset_len = test_dataset_len\n",
    "        self.config = config\n",
    "        self.tokens = 0\n",
    "        self.strategy = tf.distribute.OneDeviceStrategy(\"GPU:0\")\n",
    "        if len(tf.config.list_physical_devices('GPU')) > 1:\n",
    "            self.strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "        with self.strategy.scope():\n",
    "            self.model = model(**model_config)\n",
    "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=config.learning_rate)\n",
    "            self.cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction=tf.keras.losses.Reduction.NONE)\n",
    "            self.train_dist_dataset = self.strategy.experimental_distribute_dataset(self.train_dataset)\n",
    "            if self.test_dataset:\n",
    "                self.test_dist_dataset = self.strategy.experimental_distribute_dataset(self.test_dataset)\n",
    "\n",
    "    def save_checkpoints(self):\n",
    "        if self.config.ckpt_path is not None:\n",
    "            self.model.save_weights(self.config.ckpt_path)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
    "        test_loss_metric = tf.keras.metrics.Mean('testing_loss', dtype=tf.float32)\n",
    "\n",
    "        train_accuracy = tf.keras.metrics.Accuracy('training_accuracy', dtype=tf.float32)\n",
    "        test_accuracy = tf.keras.metrics.Accuracy('testing_accuracy', dtype=tf.float32)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(dist_inputs):\n",
    "\n",
    "            def step_fn(inputs):\n",
    "\n",
    "                X, Y = inputs\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                # training=True is only needed if there are layers with different\n",
    "                # behavior during training versus inference (e.g. Dropout).\n",
    "                    logits = self.model(X,training=True)\n",
    "                    num_labels = tf.shape(logits)[-1]\n",
    "                    label_mask = tf.math.logical_not(Y < 0)\n",
    "                    label_mask = tf.reshape(label_mask,(-1,))\n",
    "                    logits = tf.reshape(logits,(-1,num_labels))\n",
    "                    logits_masked = tf.boolean_mask(logits,label_mask)\n",
    "                    label_ids = tf.reshape(Y,(-1,))\n",
    "                    label_ids_masked = tf.boolean_mask(label_ids,label_mask)\n",
    "                    cross_entropy = self.cce(label_ids_masked, logits_masked)\n",
    "                    loss = tf.reduce_sum(cross_entropy) * (1.0 / self.config.batch_size)\n",
    "                    y_pred = tf.argmax(tf.nn.softmax(logits,axis=-1),axis=-1)\n",
    "                    train_accuracy.update_state(tf.squeeze(Y),y_pred)\n",
    "\n",
    "                grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(list(zip(grads, self.model.trainable_variables)))\n",
    "                return cross_entropy\n",
    "\n",
    "            per_example_losses = self.strategy.run(step_fn, args=(dist_inputs,))\n",
    "            sum_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\n",
    "            mean_loss = sum_loss / self.config.batch_size\n",
    "            return mean_loss\n",
    "\n",
    "        @tf.function\n",
    "        def test_step(dist_inputs):\n",
    "\n",
    "            def step_fn(inputs):\n",
    "\n",
    "                X, Y = inputs\n",
    "                # training=True is only needed if there are layers with different\n",
    "                # behavior during training versus inference (e.g. Dropout).\n",
    "                logits = self.model(X,training=False)\n",
    "                num_labels = tf.shape(logits)[-1]\n",
    "                label_mask = tf.math.logical_not(Y < 0)\n",
    "                label_mask = tf.reshape(label_mask,(-1,))\n",
    "                logits = tf.reshape(logits,(-1,num_labels))\n",
    "                logits_masked = tf.boolean_mask(logits,label_mask)\n",
    "                label_ids = tf.reshape(Y,(-1,))\n",
    "                label_ids_masked = tf.boolean_mask(label_ids,label_mask)\n",
    "                cross_entropy = self.cce(label_ids_masked, logits_masked)\n",
    "                loss = tf.reduce_sum(cross_entropy) * (1.0 / self.config.batch_size)\n",
    "                y_pred = tf.argmax(tf.nn.softmax(logits,axis=-1),axis=-1)\n",
    "                test_accuracy.update_state(tf.squeeze(Y),y_pred)\n",
    "\n",
    "                return cross_entropy\n",
    "\n",
    "            per_example_losses = self.strategy.run(step_fn, args=(dist_inputs,))\n",
    "            sum_loss = self.strategy.reduce(tf.distribute.ReduceOp.SUM, per_example_losses, axis=0)\n",
    "            mean_loss = sum_loss / self.config.batch_size\n",
    "            return mean_loss\n",
    "\n",
    "        train_pb_max_len = math.ceil(float(self.train_dataset_len)/float(self.config.batch_size))\n",
    "        test_pb_max_len = math.ceil(float(self.test_dataset_len)/float(self.config.batch_size)) if self.test_dataset else None\n",
    "\n",
    "        epoch_bar = master_bar(range(self.config.max_epochs))\n",
    "        with self.strategy.scope():\n",
    "            for epoch in epoch_bar:\n",
    "                for inputs in progress_bar(self.train_dist_dataset,total=train_pb_max_len,parent=epoch_bar):\n",
    "                    loss = train_step(inputs)\n",
    "                    self.tokens += tf.reduce_sum(tf.cast(inputs[1]>=0,tf.int32)).numpy()\n",
    "                    train_loss_metric(loss)\n",
    "                    epoch_bar.child.comment = f'training loss : {train_loss_metric.result()}'\n",
    "                print(f\"epoch {epoch+1}: train loss {train_loss_metric.result():.5f}. train accuracy {train_accuracy.result():.5f}\")\n",
    "                train_loss_metric.reset_states()\n",
    "                train_accuracy.reset_states()\n",
    "\n",
    "                if self.test_dist_dataset:\n",
    "                    for inputs in progress_bar(self.test_dist_dataset,total=test_pb_max_len,parent=epoch_bar):\n",
    "                        loss = test_step(inputs)\n",
    "                        test_loss_metric(loss)\n",
    "                        epoch_bar.child.comment = f'testing loss : {test_loss_metric.result()}'\n",
    "                    print(f\"epoch {epoch+1}: test loss {test_loss_metric.result():.5f}. test accuracy {test_accuracy.result():.5f}\")\n",
    "                    test_loss_metric.reset_states()\n",
    "                    test_accuracy.reset_states()\n",
    "\n",
    "                self.save_checkpoints()\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)                \n",
    "model_config = {image_size\":32,\n",
    "                \"patch_size\":4,\n",
    "                \"num_classes\":10,\n",
    "                \"dim\":64,\n",
    "                \"depth\":3,\n",
    "                \"heads\":4,\n",
    "                \"mlp_dim\":128}\n",
    "tconf = TrainerConfig(max_epochs=10, batch_size=64, learning_rate=1e-3)\n",
    "trainer = Trainer(ImageTransformer, model_config, train_dataset, len(train_images), test_dataset, len(test_images), tconf)\n",
    "\n",
    "trainer.train()\n",
    "#Initialize your model\n",
    "#Initialize optimizer and loss and compile it to the model\n",
    "############Write your code Here############\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "cross_entropy_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "model = ImageTransformer(image_size=32, patch_size=4, n_classes=10, batch_size=64,\n",
    "                         dimension=64, depth=3, heads=4, mlp_dimension=128)\n",
    "model.compile(optimizer=optimizer, loss_fn=cross_entropy_loss)\n",
    "\n",
    "############################################\n",
    "\n",
    "#Train your model\n",
    "############Write your code Here############\n",
    "model.fit(train_dataset, batch_size=64, epochs=10)\n",
    "############################################\n",
    "print('==============Training Finished===============')\n",
    "\n",
    "#Evaluate your test samples\n",
    "accuracy = 0\n",
    "############Write your code Here############\n",
    "\n",
    "############################################\n",
    "\n",
    "print('Test Accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DM_Assignment_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
